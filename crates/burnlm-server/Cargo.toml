[package]
name = "burnlm-server"
edition.workspace = true
version.workspace = true
license.workspace = true
readme.workspace = true

[features]
default = [
  "tch-gpu",
  "tinyllama",
]

llama3 = ["llama-burn/llama3"]
llama31 = ["llama-burn/llama3"]
tinyllama = ["llama-burn/tiny"]

# Burn backends
tch-cpu = ["burn/tch"]
tch-gpu = ["burn/tch"]
cuda = ["burn/cuda-jit"]
wgpu = ["burn/wgpu"]

[dependencies]
burn = { workspace = true }
llama-burn = { path = "../../models/llama-burn" }

axum = { workspace = true }
chrono = { workspace = true }
clap = { workspace = true }
dotenvy = { workspace = true }
lazy_static = { workspace = true }
rand = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
strum = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true }
tokio-stream = { workspace = true }
tower-http = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
utoipa = { workspace = true }
utoipa-swagger-ui = { workspace = true }

[dev-dependencies]
async-openai = "0.26.0"
rstest = { workspace = true }

[[bin]]
name = "server"
path = "src/bin/server.rs"
