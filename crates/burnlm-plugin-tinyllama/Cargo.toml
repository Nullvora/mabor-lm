[package]
name = "burnlm-plugin-tinyllama"
edition.workspace = true
version.workspace = true
license.workspace = true
readme.workspace = true

[features]
default = ["tch-gpu"]

tch-gpu = ["burn/tch", "burnlm-inference/tch-gpu"]

[dependencies]
burn = { workspace = true }
burnlm-inference = { path = "../burnlm-inference" }
llama-burn = { path = "../../models/llama-burn", features = ["tiny"] }

rand = { workspace = true }
serde = { workspace = true }

